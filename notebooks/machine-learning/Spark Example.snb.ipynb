{
  "metadata" : {
    "id" : "97d401d4-58dd-4c79-bb52-a4d25df3c050",
    "name" : "Spark Example",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "6F3E5BAE87E94ECE891B5C5065EE42FB"
    },
    "cell_type" : "markdown",
    "source" : "Pointers to directories and files."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C74B8CDCEEC349B3B1FD7A36F51709D8"
    },
    "cell_type" : "code",
    "source" : [ "import sys.process._\n", "val remoteFile = \"http://med-at-scale.s3.amazonaws.com/spark-training/dj.csv\"\n", "var dataDir = \"/tmp\"\n", "val localFile = s\"${dataDir}/dow.csv\"" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import sys.process._\nremoteFile: String = http://med-at-scale.s3.amazonaws.com/spark-training/dj.csv\ndataDir: String = /tmp\nlocalFile: String = /tmp/dow.csv\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 2.783s, at 2018-08-15 23:58"
    } ]
  }, {
    "metadata" : {
      "id" : "0FBC825BAA4741348A0904AC71B8BA80"
    },
    "cell_type" : "markdown",
    "source" : "We download this file"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "5293C9E310164BD78305E17C860EC414"
    },
    "cell_type" : "code",
    "source" : [ "s\"wget $remoteFile -O $localFile\" !!" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0CC1F85764E34BFB84619518DCA95F47"
    },
    "cell_type" : "code",
    "source" : [ "sparkContext.getConf.toDebugString" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res3: String =\nhive.metastore.warehouse.dir=file:/Users/wtnull/git/scala-project/spark-notebook/spark-warehouse/\nspark.app.id=local-1534349420824\nspark.app.name=machine-learning/Spark Example.snb.ipynb\nspark.driver.host=192.168.1.103\nspark.driver.port=59651\nspark.executor.id=driver\nspark.jars=\nspark.master=local[*]\nspark.repl.class.outputDir=/private/var/folders/xj/45w91s9s0rv0xx6vvz8ltyr40000gn/T/spark-notebook-repl-45a23320-4274-4416-86a7-c0dbdaf3e0e0\nspark.repl.class.uri=spark://192.168.1.103:59651/classes\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "hive.metastore.warehouse.dir=file:/Users/wtnull/git/scala-project/spark-notebook/spark-warehouse/\nspark.app.id=local-1534349420824\nspark.app.name=machine-learning/Spark Example.snb.ipynb\nspark.driver.host=192.168.1.103\nspark.driver.port=59651\nspark.executor.id=driver\nspark.jars=\nspark.master=local[*]\nspark.repl.class.outputDir=/private/var/folders/xj/45w91s9s0rv0xx6vvz8ltyr40000gn/T/spark-notebook-repl-45a23320-4274-4416-86a7-c0dbdaf3e0e0\nspark.repl.class.uri=spark://192.168.1.103:59651/classes"
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 2.373s, at 2018-08-16 00:18"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D33C91D41BAA497EB1E89B3E4D72C3C6"
    },
    "cell_type" : "code",
    "source" : [ "val lines = sparkContext.textFile(localFile)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "lines: org.apache.spark.rdd.RDD[String] = /tmp/dow.csv MapPartitionsRDD[3] at textFile at <console>:73\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 1.259s, at 2018-08-15 23:58"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5075465245854DF382FA575EABF59591"
    },
    "cell_type" : "code",
    "source" : [ "lines.take(4)" ],
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/tmp/dow.csv\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1333)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1327)\n  ... 65 elided\n"
    } ]
  }, {
    "metadata" : {
      "id" : "B3B823ED0D804BDA8A8986CEC4F1F141"
    },
    "cell_type" : "markdown",
    "source" : "The case classes we need to define the schema and parsers"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "D87C6BF54D4A49728D5320292426D9F9"
    },
    "cell_type" : "code",
    "source" : [ "object model extends Serializable {\n", "\n", "  object MyDate {\n", "    val df = new java.text.SimpleDateFormat(\"yyyy-MM-dd\")\n", "    def parse(field: String): Option[MyDate] = {\n", "      try {\n", "        val ts = df.parse(field).getTime\n", "        field.split(\"-\").map(_.toInt).toList match {\n", "          case year :: month :: day :: _ => Some(MyDate(year, month, day, ts))\n", "          case _ => None\n", "        }\n", "      } catch {\n", "        case ex: Throwable => \n", "          Console.err.println(s\"$ex: datefield = $field\")\n", "        None\n", "      }\n", "    }\n", "  }\n", "  case class MyDate(year: Int, month: Int, day: Int, timestamp: Long)\n", "  \n", "\n", "  object Quote {\n", "    def parse(line: String): Option[Quote] = {\n", "      val fields = line.trim.split(\"\"\"\\s*,\\s*\"\"\")\n", "      try {\n", "        MyDate.parse(fields(1)).map { date => Quote(fields(0), date, fields(2).toDouble)}                \n", "      } catch {\n", "        case ex: NumberFormatException =>\n", "          Console.err.println(s\"$ex: line = $line\")\n", "          None\n", "        case ex: IndexOutOfBoundsException =>\n", "          Console.err.println(s\"$ex: line = $line\")\n", "          None\n", "      }\n", "    }\n", "  }\n", "  case class Quote(stock:String, date:MyDate, price:Double)\n", "\n", "}\n", "import model._" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "13FC0FB31B664C2FB8F771467171ED11"
    },
    "cell_type" : "markdown",
    "source" : "Parsing the file and convert to Quote objects"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "240B14D4B04140B4B488AFE6F7A1D649"
    },
    "cell_type" : "code",
    "source" : [ "val quotes = lines.map(Quote.parse).collect{case Some(q) => q}" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "994118F75BF24CA0ABC3955C4863A10B"
    },
    "cell_type" : "markdown",
    "source" : "Import SQLContext (wrapper around SparkContext to give access to sparkSQL functions)\nCreate the SQLContext\nLoad some implicit functions "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "ADEF293AF16E4AF08545B69A3ED653A6"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.SQLContext\n", "val sqlContext = new SQLContext(sparkContext)\n", "import sqlContext.implicits._ " ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "C82EC9566C884D72869C7CB0FD265F1D"
    },
    "cell_type" : "markdown",
    "source" : "Now we crrate a dataframe from the RDD, schema is built from the case class definition, including nested structure"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "C6D0504DE256436C85DA677CCB043E53"
    },
    "cell_type" : "code",
    "source" : [ "val quotesdf = sqlContext.createDataFrame(quotes)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "A978AE20769B4231977AFBA44E9D2F49"
    },
    "cell_type" : "markdown",
    "source" : "Print the Dataframe schema"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "8038501EC380491683EA6A25D1005EC5"
    },
    "cell_type" : "code",
    "source" : [ "quotesdf.printSchema" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "AF0A9A5B840F493F83CA0D3B0CECA139"
    },
    "cell_type" : "markdown",
    "source" : "We change the column names by creating a new dataframe"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "EBF7268A71344E82A1683BDFA71F7C72"
    },
    "cell_type" : "code",
    "source" : [ "val ts = quotesdf.select(\"date.timestamp\").map(_.getAs[Long](0)).distinct.collect.toList.sorted\n", "val withNextTs = ts.sliding(2, 1).map(x => (x(0) → x(1))).toList" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "6E73AB9E3E8E43789E7099969591E605"
    },
    "cell_type" : "code",
    "source" : [ "quotesdf.count" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "02E8BDEFDBE74BCB83286EC8F2028C25"
    },
    "cell_type" : "code",
    "source" : [ "val scoped = new Serializable {\n", "  \n", "  @transient val wnt = withNextTs\n", "  \n", "  val bc = sparkContext.broadcast(wnt)\n", "  \n", "  val updatedQuotes:Dataset[(String, Long, Double)] = \n", "      quotesdf\n", "      .withColumn(\"ts\", $\"date.timestamp\")\n", "      .flatMap { row: Row =>\n", "        val ts = row.getAs[Long](3)\n", "        val wnt = bc.value\n", "        wnt.find(_._2 == ts) match {\n", "          case None => List.empty[(String, Long, Double)]\n", "          case Some((previousTs, _)) => \n", "            val thisUpdated = (row.getAs[String](0), ts, -1 * row.getAs[Double](2)) \n", "            val newRow = (row.getAs[String](0), previousTs, row.getAs[Double](2)) \n", "            \n", "            if (!wnt.find(_._1 == ts).isDefined)\n", "              List(newRow)\n", "            else if (!wnt.find(_._2 == previousTs).isDefined)\n", "              List(thisUpdated)\n", "            else\n", "              List(newRow, thisUpdated)\n", "        }\n", "      }\n", "  val df = updatedQuotes.toDF(\"symbol\", \"ts\", \"close\")\n", "}\n", "val duplicatedWithPreviousTs = scoped.df\n", "\n", "import org.apache.spark.sql.functions._\n", "val diffQuotes = duplicatedWithPreviousTs.groupBy(\"symbol\", \"ts\").agg(sum(\"close\").as(\"diff_close\"))" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "9CDDDF2F08C8428EBD7EA875365D4D42"
    },
    "cell_type" : "code",
    "source" : [ ":markdown \n", "We have ${diffQuotes.count} element" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "C3A6D42E11C145CB8768C850EA8FBBD8"
    },
    "cell_type" : "markdown",
    "source" : "Write data to parquet and json"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "4BC14D1E9C2D4ACEA131D5F338448816"
    },
    "cell_type" : "code",
    "source" : [ "diffQuotes.write.parquet(s\"$dataDir/dow.parquet\")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "60A6F8F0BB7E4996BD52602ADB22BAE9"
    },
    "cell_type" : "code",
    "source" : [ "diffQuotes.write.json(s\"$dataDir/quotes.json\")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "6A24CD5D945545D78D51DC1E04321411"
    },
    "cell_type" : "markdown",
    "source" : "Filter rows"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "D2A8DC77650347918CDBFEA2AD9F5AEC"
    },
    "cell_type" : "code",
    "source" : [ "val ibm = diffQuotes.filter($\"symbol\" === \"IBM\" && $\"diff_close\" < -10).orderBy($\"diff_close\".asc)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "9D20757DC91042EEA38C7BEFE197FFFD"
    },
    "cell_type" : "code",
    "source" : [ "val ko = diffQuotes.filter($\"symbol\" === \"KO\" && $\"diff_close\" < -10)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "A463115EF50B4BD58534BA16A5729789"
    },
    "cell_type" : "code",
    "source" : [ "quotesdf.filter($\"stock\" === \"KO\").agg(max(\"price\"), min(\"price\"))" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B513D28452074AB3B5A68BBD51913FF9"
    },
    "cell_type" : "markdown",
    "source" : "Dataframes can be cached too"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "B0E7D9019C8F4601A5087B5022A3B97A"
    },
    "cell_type" : "code",
    "source" : [ "ibm.cache()" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "1CC6E51B6C0241968459AB8EE4AE693D"
    },
    "cell_type" : "markdown",
    "source" : "Create a grouping"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "76207687F13F4FBE98BA0A1D44B447EA"
    },
    "cell_type" : "code",
    "source" : [ "val bySymbol = diffQuotes.groupBy(\"symbol\")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "22DF58DCBA5B43BDB11F1374C3A58F91"
    },
    "cell_type" : "markdown",
    "source" : "Apply some aggregation on the grouping"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "7B75B964A19E47FB8297B61C05A55A25"
    },
    "cell_type" : "code",
    "source" : [ "bySymbol.count.orderBy($\"count\".desc)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "5DE3F348FCA94528BD3E50FFF1A45B48"
    },
    "cell_type" : "markdown",
    "source" : "Apply several aggregations on the grouping"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "48B1433489FF4C0B9A3D18623E8B5BFE"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.sql.functions._\n", "bySymbol.agg(\n", "  count(\"diff_close\").as(\"count\"), \n", "  min(\"diff_close\").as(\"min\"), \n", "  max(\"diff_close\").as(\"max\"), \n", "  mean(\"diff_close\").as(\"avg.\")\n", ")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "F251D6678330448CA28BC64E4D2511B7"
    },
    "cell_type" : "markdown",
    "source" : "### Now work with SQL..."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "29E65B175EF8451A9D13BF73D9A8B3DA"
    },
    "cell_type" : "code",
    "source" : [ "val data = sqlContext.read.parquet(s\"$dataDir/dow.parquet\")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "2722F597C93042E081A2DC9E5FEDDE23"
    },
    "cell_type" : "code",
    "source" : [ "data.registerTempTable(\"quote\")\n", "data.cache()\n", "()" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "F384DEB0D630446B8C403646FADB79A8"
    },
    "cell_type" : "code",
    "source" : [ "sqlContext.sql(\"\"\"\n", "  SELECT s.symbol, s.ts, s.diff_close \n", "  FROM quote s \n", "  WHERE symbol = 'IBM' ORDER BY s.ts ASC\n", "\"\"\")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "7B35228B28DB4B8BA032F22630AE0B18"
    },
    "cell_type" : "code",
    "source" : [ "sqlContext.sql(\"\"\"\n", " SELECT q.symbol AS symbol, count(*) as count \n", " FROM quote q GROUP BY q.symbol \n", " ORDER BY count DESC\n", "\"\"\")" ],
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}